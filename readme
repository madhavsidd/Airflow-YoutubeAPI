# Twitter Data Analysis using Airflow

This project automates the extraction and analysis of YouTube comments using an ETL pipeline built with Apache Airflow. The goal is to gather insights from the comments and help YouTube creators improve their content.

## Documentation

This repository contains multiple aspects of the project, divided into the following sections:

1. [Why I Started This Project](./docs/why_i_started.md)
2. [API Setup](./docs/api_setup.md)
3. [Data Extraction](./docs/data_extraction.md)
4. [Data Transformation](./docs/data_transformation.md)
5. [Data Loading](./docs/data_loading.md)
6. [Using ChatGPT for Analysis](./docs/chatgpt_analysis.md)

## How to Run the Project

To run this project locally:
1. Clone the repository.
2. Install the necessary dependencies.
3. Set up Airflow with Docker.

For detailed instructions, see the [Setup Guide](./docs/setup_guide.md).

# Why I Started This Project

This project was born out of the desire to gather actionable insights from comments that could help content creators enhance their videos.

# API Setup

## YouTube Data API

Google provides a powerful API for interacting with YouTube data. I used the [YouTube Data API](https://developers.google.com/youtube/v3/docs) for extracting comments from a specific video.

### Steps to Set Up

1. Go to the [Google Developer Console](https://console.cloud.google.com/).
2. Create a new project and enable the YouTube Data API.
3. Generate an API key and use it in the following starter code:

```python
import os
import googleapiclient.discovery

---

### `docs/data_extraction.md`

# Data Extraction

Once the YouTube API was set up, I used it to extract comments from the video. Below is an example of how a comment looks:

```json
{
  "items": [
    {
      "kind": "youtube#commentThread",
      "id": "UgwG6AJegM6Xg6EGW254AaABAg",
      "snippet": {
        "topLevelComment": {
          "snippet": {
            "authorDisplayName": "Saliyuk Benjamin",
            "textOriginal": "Getting the below error, can anyone help?",
            "publishedAt": "2023-08-13T23:06:27Z"
          }
        }
      }
    }
  ]
}

---

### `docs/data_transformation.md`

# Data Transformation

After extracting the comments, I focused on cleaning and structuring the data. The main transformation steps included:

1. Parsing the JSON response to extract only relevant fields.
2. Organizing the data into a pandas DataFrame for easier analysis and export to a CSV file.

---

### `docs/data_loading.md`

# Data Loading

The final step in the pipeline was loading the extracted and transformed data into a CSV file.

I used Pandas to write the data into a well-structured CSV, which could be used for further analysis or shared with others.

---

### `docs/chatgpt_analysis.md`

# Using ChatGPT for Analysis

Once I had the data in a CSV format, I wanted to analyze it further. For this, I turned to ChatGPT.

Using ChatGPT, I was able to quickly summarize the main topics discussed in the comments and gather insights that would be valuable for the content creator.

## Prompt Engineering

I learned the basics of prompt engineering from the [Learn Prompting](https://learnprompting.org/docs/category/-basics) guide, which helped me structure my prompts to get the most accurate and relevant responses.
