# Twitter Data Analysis using Airflow

This project automates the extraction and analysis of YouTube comments using an ETL pipeline built with Apache Airflow. The goal is to gather insights from the comments and help YouTube creators improve their content.

## Documentation

This repository contains multiple aspects of the project, divided into the following sections:

1. [Why I Started This Project](./docs/why_i_started.md)
2. [API Setup](./docs/api_setup.md)
3. [Data Extraction](./docs/data_extraction.md)
4. [Data Transformation](./docs/data_transformation.md)
5. [Data Loading](./docs/data_loading.md)
6. [Using ChatGPT for Analysis](./docs/chatgpt_analysis.md)

## How to Run the Project

To run this project locally:
1. Clone the repository.
2. Install the necessary dependencies.
3. Set up Airflow with Docker.

For detailed instructions, see the [Setup Guide](./docs/setup_guide.md).

# Why I Started This Project

I came across a YouTube tutorial that taught me how to create a simple data pipeline. After completing the tutorial, I decided to give back to the YouTuber who helped me learn so much. I realized I could build a simple ETL pipeline to analyze YouTube comments using the Google Data API.

This project was born out of the desire to gather actionable insights from comments that could help content creators enhance their videos.

Check out the tutorial here: [Twitter Data Pipeline using Airflow for Beginners](https://www.youtube.com/watch?v=q8q3OFFfY6c&t=1362s).

# API Setup

## YouTube Data API

Google provides a powerful API for interacting with YouTube data. I used the [YouTube Data API](https://developers.google.com/youtube/v3/docs) for extracting comments from a specific video.

### Steps to Set Up

1. Go to the [Google Developer Console](https://console.cloud.google.com/).
2. Create a new project and enable the YouTube Data API.
3. Generate an API key and use it in the following starter code:

```python
import os
import googleapiclient.discovery

def main():
    api_service_name = "youtube"
    api_version = "v3"
    DEVELOPER_KEY = "YOUR_API_KEY"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)

    request = youtube.commentThreads().list(
        part="snippet, replies",
        videoId="YOUR_VIDEO_ID"
    )
    response = request.execute()

    print(response)

if __name__ == "__main__":
    main()



---

### 4. **Data Extraction (`data_extraction.md`)**

```markdown
# Data Extraction

Once the YouTube API was set up, I used it to extract comments from the video. Below is an example of how a comment looks:

```json
{
  "items": [
    {
      "kind": "youtube#commentThread",
      "id": "UgwG6AJegM6Xg6EGW254AaABAg",
      "snippet": {
        "topLevelComment": {
          "snippet": {
            "authorDisplayName": "Saliyuk Benjamin",
            "textOriginal": "Getting the below error, can anyone help?",
            "publishedAt": "2023-08-13T23:06:27Z"
          }
        }
      }
    }
  ]
}



---

### 5. **Data Transformation (`data_transformation.md`)**

```markdown
# Data Transformation

After extracting the comments, I focused on cleaning and structuring the data. The main transformation steps included:

1. Parsing the JSON response to extract only relevant fields.
2. Organizing the data into a pandas DataFrame for easier analysis and export to a CSV file.

```python
import pandas as pd

comments_df = pd.DataFrame(comments)
comments_df.to_csv('youtube_comments.csv', index=False)



---

### 6. **Data Loading (`data_loading.md`)**

```markdown
# Data Loading

The final step in the pipeline was loading the extracted and transformed data into a CSV file.

I used Pandas to write the data into a well-structured CSV, which could be used for further analysis or shared with others.

```python
comments_df.to_csv('youtube_comments.csv', index=False)



---

### 7. **Using ChatGPT for Analysis (`chatgpt_analysis.md`)**

```markdown
# Using ChatGPT for Analysis

Once I had the data in a CSV format, I wanted to analyze it further. For this, I turned to ChatGPT.

Using ChatGPT, I was able to quickly summarize the main topics discussed in the comments and gather insights that would be valuable for the content creator.

## Prompt Engineering

I learned the basics of prompt engineering from the [Learn Prompting](https://learnprompting.org/docs/category/-basics) guide, which helped me structure my prompts to get the most accurate and relevant responses.
